t5:
  pretrained_model_name_or_path: 'QizhiPei/biot5-plus-base'
swin:
  img_size: [224, 224]
  num_classes: 0
  embed_dim: 192
  depths: [2,2,18,2]
  num_heads: [6,12,24,48]
  pretrained_model_path: 'weights/swin_transform_focalloss.pth'
roberta:
  pretrained_model_name_or_path: 'allenai/scibert_scivocab_uncased'
momu:
  "name": "MoMu"
  "gin_hidden_dim": 300
  "gin_num_layers": 5
  "drop_ratio": 0.0
  "graph_pooling": "sum"
  "graph_self": False
  "max_n_nodes": -1
  "bert_hidden_dim": 1024
  "bert_dropout": 0.0
  "output_dim": 300
  "projection_dim": 256
  "param_key": 'state_dict'
  "stop_grad": False
  pretrained_model_path: 'weights/graph_kvplm_pretrain.pt'
multimodal:
  n_attention_heads: 8
  fusion_encoder_layers: [10,11]
  fusion_decoder_layers: [0,1]
  use_visual_feature: true
  trainable_visual: false
  use_smiles_feature: true
  trainable_smiles: false
  use_graph_feature: true
  trainable_graph: false
  use_forget_gate: true
  visual_feature_dim: 1536
  text_feature_dim: 768
  smiles_feature_dim: 768
  graph_feature_dim: 300
  intermediate_dim: 256